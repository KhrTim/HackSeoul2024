{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca268d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 을 통해 Google API를 사용하기 위해서 라이브러리 다운로드\n",
    "!pip install google-api-python-client\n",
    "# Google 과 관련한 API를 사용하기 위해 필요한 모듈\n",
    "!pip install google-auth-oauthlib google-auth-httplib2\n",
    "# Youtube Captions 를 추출하기 위한 API를 사용하기 위해서 라이브러리 다운로드\n",
    "!pip install youtube-transcript-api\n",
    "!pip install nltk\n",
    "!pip install vaderSentiment\n",
    "!pip install spacy\n",
    "!pip install textblob\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# OpenAI API 클라이언트 생성\n",
    "openai.api_key = 'YOUR_KEY'  # 자신의 OpenAI API 키 입력\n",
    "\n",
    "# YouTube API 클라이언트 생성\n",
    "api_key = 'YOUR_KEY'  # 자신의 YouTube API 키 입력\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 감정 분석기 초기화\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 주제 관련 비디오 검색 함수\n",
    "def search_videos_by_keyword(keyword, region_code='KR', max_results=3):\n",
    "    numbers = [5, 10, 30, 50]  # 사용할 숫자 목록\n",
    "    prefixes = [\"Top\", \"Ranking\"]  # 사용할 접두사 목록\n",
    "\n",
    "    six_months_ago = datetime.now() - timedelta(days=180)\n",
    "    published_after = six_months_ago.isoformat(\"T\") + \"Z\"  # ISO 8601 형식으로 변환\n",
    "\n",
    "    search_results = {}\n",
    "\n",
    "    for number in numbers:\n",
    "        for prefix in prefixes:\n",
    "            query = f\"{prefix} {number} {keyword} review\"\n",
    "\n",
    "            search_request = youtube.search().list(\n",
    "                part='snippet',\n",
    "                type='video',\n",
    "                q=query,\n",
    "                regionCode=region_code,\n",
    "                maxResults=max_results,\n",
    "                publishedAfter=published_after\n",
    "            )\n",
    "\n",
    "            search_response = search_request.execute()\n",
    "\n",
    "            video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
    "\n",
    "            video_request = youtube.videos().list(\n",
    "                part='snippet,statistics',\n",
    "                id=','.join(video_ids)\n",
    "            )\n",
    "\n",
    "            video_response = video_request.execute()\n",
    "\n",
    "            for item in video_response['items']:\n",
    "                video_id = item['id']\n",
    "                title = item['snippet']['title']\n",
    "                view_count = int(item['statistics']['viewCount'])\n",
    "                published_date = item['snippet']['publishedAt']\n",
    "\n",
    "                try:\n",
    "                    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "                    formatter = TextFormatter()\n",
    "                    captions = formatter.format_transcript(transcript)\n",
    "                    captions = split_into_sentences(captions)\n",
    "                except Exception as e:\n",
    "                    captions = f\"No captions available or error occurred: {e}\"\n",
    "\n",
    "                search_results[video_id] = {\n",
    "                    'title': title,\n",
    "                    'views': view_count,\n",
    "                    'published_date': published_date,\n",
    "                    'captions': captions\n",
    "                }\n",
    "\n",
    "    return search_results\n",
    "\n",
    "# 자막을 문장 단위로 분리하는 함수\n",
    "def split_into_sentences(captions):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', captions)\n",
    "    return \" \".join(sentences)\n",
    "\n",
    "# OpenAI GPT-4 모델을 사용하여 제품명 추출 함수\n",
    "def extract_product_names_with_gpt(captions):\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            prompt=f\"Extract product names from the following text: {captions}\",\n",
    "            max_tokens=100,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        product_names = response.choices[0].text.strip().split(\",\")\n",
    "        product_names = [name.strip() for name in product_names]\n",
    "        return product_names\n",
    "    except Exception as e:\n",
    "        print(f\"Error in GPT-4o API call: {e}\")\n",
    "        return []\n",
    "\n",
    "# 제품 리뷰 자막에서 주요 특징(Aspect) 추출 함수\n",
    "def extract_aspects(captions, top_n=5):\n",
    "    doc = nlp(captions)\n",
    "    nouns = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(nouns)\n",
    "    tfidf_scores = dict(zip(vectorizer.get_feature_names_out(), X.sum(axis=0).tolist()[0]))\n",
    "    sorted_tfidf = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    aspects = [aspect for aspect, score in sorted_tfidf[:top_n]]\n",
    "    return aspects\n",
    "\n",
    "# ABSA(Aspect-Based Sentiment Analysis) 함수\n",
    "def analyze_sentiment(captions, aspects):\n",
    "    aspect_sentiments = {}\n",
    "    for aspect in aspects:\n",
    "        sentences = [sent for sent in captions.split('. ') if aspect in sent]\n",
    "        sentiment_scores = [analyzer.polarity_scores(sent)['compound'] for sent in sentences]\n",
    "        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
    "        aspect_sentiments[aspect] = avg_sentiment\n",
    "    return aspect_sentiments\n",
    "\n",
    "# 제품 자체의 감정 점수를 계산하는 함수 (특성의 감정 점수 기반)\n",
    "def calculate_product_sentiment_from_aspects(sentiment_scores, weight=1.0):\n",
    "    if sentiment_scores:\n",
    "        avg_sentiment = (sum(sentiment_scores.values()) / len(sentiment_scores)) * weight\n",
    "    else:\n",
    "        avg_sentiment = 0\n",
    "    return avg_sentiment\n",
    "\n",
    "# 자막 데이터를 기반으로 제품명, 특징 및 감성 분석을 수행하는 함수\n",
    "def process_single_caption(video_id, captions, weight=1.0):\n",
    "    print(f\"\\nProcessing Video ID: {video_id}\")\n",
    "    product_names = extract_product_names_with_gpt(captions)\n",
    "\n",
    "    if not product_names:\n",
    "        print(\"No product names found.\")\n",
    "        return\n",
    "\n",
    "    product_sentiments = {}\n",
    "\n",
    "    print(\"Extracted product names:\")\n",
    "    for product in product_names:\n",
    "        print(f\"Product: {product}\")\n",
    "\n",
    "    for product in product_names:\n",
    "        print(f\"\\nAnalyzing product: {product}\")\n",
    "        product_section = re.findall(f\"{product}.*?(?=\\\\n[A-Z]|$)\", captions, re.DOTALL)\n",
    "        if product_section:\n",
    "            product_section = product_section[0]\n",
    "            aspects = extract_aspects(product_section)\n",
    "            sentiment_scores = analyze_sentiment(product_section, aspects)\n",
    "            product_sentiment = calculate_product_sentiment_from_aspects(sentiment_scores, weight)\n",
    "\n",
    "            product_sentiments[product] = product_sentiment\n",
    "\n",
    "            print(f\"\\nProduct Sentiment: {product_sentiment}\")\n",
    "            print(\"\\nExtracted aspects and their sentiment scores:\")\n",
    "            for aspect, sentiment in sentiment_scores.items():\n",
    "                print(f\"Aspect: {aspect}, Sentiment: {sentiment}\")\n",
    "        else:\n",
    "            print(f\"No detailed section found for product: {product}\")\n",
    "\n",
    "    sorted_products = sort_products_by_sentiment(product_sentiments)\n",
    "    print(\"\\nProducts sorted by sentiment (descending):\")\n",
    "    for product, sentiment in sorted_products:\n",
    "        print(f\"Product: {product}, Sentiment: {sentiment}\")\n",
    "\n",
    "# 여러 동영상 자막을 처리하는 함수\n",
    "def process_multiple_captions(captions_data, weights):\n",
    "    final_results = []\n",
    "    for video_id, data in captions_data.items():\n",
    "        captions = data['captions']\n",
    "        weight = weights.get(video_id, 1.0)\n",
    "        process_single_caption(video_id, captions, weight)\n",
    "        final_results.append((video_id, data['title'], weight))\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# 가중치를 동적으로 계산하는 함수 (조회수 기반)\n",
    "def calculate_dynamic_weights(captions_data):\n",
    "    max_views = max(data['views'] for data in captions_data.values())\n",
    "    weights = {}\n",
    "    for video_id, data in captions_data.items():\n",
    "        view_count = data['views']\n",
    "        weights[video_id] = view_count / max_views\n",
    "    return weights\n",
    "\n",
    "# 제품 및 감정 점수 정렬 함수\n",
    "def sort_products_by_sentiment(product_sentiments):\n",
    "    return sorted(product_sentiments.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 최종 결과 출력 및 정렬 함수\n",
    "def print_final_results(final_results):\n",
    "    sorted_results = sorted(final_results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(\"\\nFinal Results (Sorted by Product Sentiment):\")\n",
    "    for video_id, title, sentiment in sorted_results:\n",
    "        print(f\"Video ID: {video_id}, Title: {title}, Sentiment: {sentiment}\")\n",
    "\n",
    "# 검색어로 비디오 검색 및 자막 데이터 가져오기\n",
    "keyword = category # 'Mechanical Keyboard\n",
    "captions_data = search_videos_by_keyword(keyword)\n",
    "\n",
    "# 가중치 계산\n",
    "weights = calculate_dynamic_weights(captions_data)\n",
    "\n",
    "# 여러 자막 데이터 처리 후 최종 결과 출력\n",
    "final_results = process_multiple_captions(captions_data, weights)\n",
    "print_final_results(final_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
